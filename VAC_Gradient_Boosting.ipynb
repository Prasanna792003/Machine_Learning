{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "There are a number of excellent reasons:\n",
        "\n",
        "Gradient boosting is the best: its accuracy and performance are unmatched for tabular supervised learning tasks.\n",
        "Gradient boosting is highly versatile: it can be used in many important tasks such as regression, classification, ranking, and survival analysis.\n",
        "Gradient boosting is interpretable: unlike black-box algorithms like neural networks, gradient boosting does not sacrifice interpretability for performance. It works like a Swiss watch and yet, with patience, you can teach how it works to a school kid.\n",
        "Gradient boosting is well-implemented: it is not one of those algorithms that have little practical value. Various gradient boosting libraries like XGBoost and LightGBM in Python are used by hundreds of thousands of people.\n",
        "Gradient boosting wins: since 2015, professionals have used it to consistently win tabular competitions on platforms like Kaggle."
      ],
      "metadata": {
        "id": "QBy193BzZ43p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "import Libraries"
      ],
      "metadata": {
        "id": "8tUkZWZAaEJP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "fhXg6nHHZ29S"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import cross_val_score, train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the diamonds dataset from Seaborn\n",
        "diamonds = sns.load_dataset(\"diamonds\")\n",
        "\n",
        "# Split data into features and target\n",
        "X = diamonds.drop(\"cut\", axis=1)\n",
        "y = diamonds[\"cut\"]"
      ],
      "metadata": {
        "id": "M0630GliaLw2"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "   X, y, test_size=0.2, random_state=42\n",
        ")"
      ],
      "metadata": {
        "id": "nWQ2rRs-aRw4"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define categorical and numerical features\n",
        "categorical_features = X.select_dtypes(\n",
        "   include=[\"object\"]\n",
        ").columns.tolist()\n",
        "\n",
        "numerical_features = X.select_dtypes(\n",
        "   include=[\"float64\", \"int64\"]\n",
        ").columns.tolist()"
      ],
      "metadata": {
        "id": "UkYMShlgaWkR"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define preprocessing steps for categorical and numerical features"
      ],
      "metadata": {
        "id": "A1nCo5N6apge"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessor = ColumnTransformer(\n",
        "   transformers=[\n",
        "       (\"cat\", OneHotEncoder(), categorical_features),\n",
        "       (\"num\", StandardScaler(), numerical_features),\n",
        "   ]\n",
        ")"
      ],
      "metadata": {
        "id": "WRvcmnuFat8I"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a Gradient Boosting Classifier pipeline"
      ],
      "metadata": {
        "id": "1Zr4kTMhacvR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline = Pipeline(\n",
        "   [\n",
        "       (\"preprocessor\", preprocessor),\n",
        "       (\"classifier\", GradientBoostingClassifier(random_state=42)),\n",
        "   ]\n",
        ")\n"
      ],
      "metadata": {
        "id": "KLuIkjfFaZ9d"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CV and training"
      ],
      "metadata": {
        "id": "lf0DElLOazbf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform 5-fold cross-validation\n",
        "cv_scores = cross_val_score(pipeline, X_train, y_train, cv=5)\n",
        "\n",
        "# Fit the model on the training data\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = pipeline.predict(X_test)\n",
        "\n",
        "# Generate classification report\n",
        "report = classification_report(y_test, y_pred)\n"
      ],
      "metadata": {
        "id": "QcxYgVFqa2mE"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Report the final results"
      ],
      "metadata": {
        "id": "nKln565oa7ft"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Mean Cross-Validation Accuracy: {cv_scores.mean():.4f}\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(report)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MTvuNkoYa9X1",
        "outputId": "73d19fc8-b846-42c1-f499-e9a8b799d3a7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Cross-Validation Accuracy: 0.7621\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Fair       0.90      0.91      0.91       335\n",
            "        Good       0.81      0.63      0.71      1004\n",
            "       Ideal       0.82      0.91      0.86      4292\n",
            "     Premium       0.70      0.86      0.77      2775\n",
            "   Very Good       0.66      0.41      0.51      2382\n",
            "\n",
            "    accuracy                           0.76     10788\n",
            "   macro avg       0.78      0.74      0.75     10788\n",
            "weighted avg       0.75      0.76      0.75     10788\n",
            "\n"
          ]
        }
      ]
    }
  ]
}